---
title: "Some changepoint simulations"
author: "P. Fearnhead & G. Rigaill"
date: "26 Mars 2018"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Summary & Parameters

Here we explore some segmentation approaches and their performances on various datasets.
This is a draft.

```{r}
mc.cores=3
nb_simu_per_dataset=100
```

## Simulated datasets

We will test a number of segmentation approaches on various simulated datasets.
For each datasets we need to define a number of parameters 

- a name
- the mean of each vector (mu)
- the changes (bkp)
- the length of each segment (Lg)
- the standard deviation (sigma)
- the true number of changes (Ktrue)
- the signal (signal)

We store the parameter of each dataset in a list. Here is 
the code for our first dataset.

```{r dataset1, fig.height=5}
Simu <- list()
isimu <- 0

isimu <- isimu+1
Simu[[isimu]] <- list()
Simu[[isimu]]$Name <- "Dataset-1, K=12, n=2048"
Simu[[isimu]]$bkp  <- c(0, c(205, 267, 308, 472, 512, 820, 902, 1332, 1557, 
                                  1598, 1659) - 1, 2048)
Simu[[isimu]]$Lg <- diff(Simu[[isimu]]$bkp)
Simu[[isimu]]$mu <- c(0, 14.64, -3.66, 7.32, -7.32, 10.98, -4.39, 
                      3.29, 19.03,7.68, 15.37, 0)
Simu[[isimu]]$sigma=10; 
Simu[[isimu]]$signal <- rep(Simu[[isimu]]$mu, Simu[[isimu]]$Lg); 
Simu[[isimu]]$Ktrue <- sum(diff(Simu[[isimu]]$signal)!=0)+1

```

The defintion of all other datasets is in the "StatDatasets.R" file.
We source them below.

```{r statdatasets}
source("StatDatasets.R")
```

### An example profile for each dataset

We now provide a function simulate profiles for a given dataset.

```{r plot_dataset, fig.width=4, fig.height=3.4}
library(stats)
Sim.method <- list()
i <- 0

##### Gauss
i <- i+1
Sim.method[[i]] <- list()
Sim.method[[i]]$Name <- "Gauss"
Sim.method[[i]]$FUN <- function(sim_){
   x <- rep(sim_$mu, sim_$Lg) + rnorm(sum(sim_$Lg), sd=sim_$sigma)
   return(x)
}
##### Student
i <- i+1
Sim.method[[i]] <- list()
Sim.method[[i]]$Name <- "Stud-10"
Sim.method[[i]]$FUN <- function(sim_){
    dfree <- 10
    x <- rep(sim_$mu, sim_$Lg) 
    x <- x + rt(n=length(x), df=dfree)*sim_$sigma
    return(x)
}
#### ARMA
i <- i+1
Sim.method[[i]] <- list()
Sim.method[[i]]$Name <- "ARMA"
Sim.method[[i]]$FUN <- function(sim_){
    arima.model <- list(order=c(1,0,0), ar=0.3)
    x <- rep(sim_$mu, sim_$Lg) 
    x <- x + arima.sim(arima.model,n=length(x))*sim_$sigma
    return(as.vector(x))
}

```


#### Gaussian error profile examples

```{r plot_Gauss_profile, fig.width=4, fig.height=3.4}
for(sim in Simu){
  x <- Sim.method[[1]]$FUN(sim)
  plot(x, pch=20, col="blue", lty=2, cex=0.3, xlab="", ylab="",
       main=paste0(sim$Name, ", ", Sim.method[[1]]$Name))
  abline(v=sim$bkp, lty=2)
  segments(x0=sim$bkp[-length(sim$bkp)], y0=sim$mu, sim$bkp[-1], col="red", lwd=2)
}
```

#### Student error profile examples

```{r plot_Student_profile, fig.width=4, fig.height=3.4}
for(sim in Simu){
 x <- Sim.method[[2]]$FUN(sim)
  plot(x, pch=20, col="blue", lty=2, cex=0.3, xlab="", ylab="",
       main=paste0(sim$Name, ", ", Sim.method[[2]]$Name))
  abline(v=sim$bkp, lty=2)
  segments(x0=sim$bkp[-length(sim$bkp)], y0=sim$mu, sim$bkp[-1], col="red", lwd=2)
}
```

#### ARMA error profile examples

```{r plot_ARMA_profile, fig.width=4, fig.height=3.4}
for(sim in Simu){
 x <- Sim.method[[3]]$FUN(sim)
  plot(x, pch=20, col="blue", lty=2, cex=0.3, xlab="", ylab="",
       main=paste0(sim$Name, ", ", Sim.method[[3]]$Name))
  abline(v=sim$bkp, lty=2)
  segments(x0=sim$bkp[-length(sim$bkp)], y0=sim$mu, sim$bkp[-1], col="red", lwd=2)
}
```

### Larger datasets 

Finally we consider a few more datasets extending the previous one by
multiplying the length of each segment by $\alpha$ and dividing the standard 
deviation by $\sqrt(\alpha)$. We use $\alpha=3$ and $\alpha=10$
This is done in the following script.

```{r extended_datasets}
one_Extend_Simu_Lg <- function(sim_, alpha_){
  sim_$bkp    <- sim_$bkp*alpha_
  sim_$Lg     <- diff(sim_$bkp)
  sim_$sigma  <- sim_$sigma*sqrt(alpha_)
  sim_$signal <- rep(sim_$mu, sim_$Lg); 
  sim_$Name   <- paste0(gsub(",.*", "", sim_$Name), ", x", alpha_, ", K=", sim_$Ktrue, ", n=", length(sim_$signal))
  return(sim_)
}

Simu_Ext <- lapply(Simu, FUN=function(sim_, alphas_=c(1, 3, 10)){
  new_sim <- lapply(alphas_, FUN=one_Extend_Simu_Lg, sim=sim_)
  return(new_sim)
})

```

### Simulation

We now simulate a number (100) profile per dataset and store them in RData.

```{r simulation_save}
set.seed(100) ## fix the seed so that regenerating data is easy
storeSim <- function(sim_, method_, ns_=nb_simu_per_dataset){
  file_sim <- paste0("Simu_Profile/Error=", method_$Name, "_", gsub(",[ ]", "_", sim_$Name), ".RData")
  
  if(!file.exists(file_sim)){
    data_sim <- replicate(ns_, method_$FUN(sim_))
    save(data_sim, file=file_sim, compress = TRUE)
  }
}

## This is about 110 Mo for alpha 1, 3, 100 and one type of error
for(method in Sim.method){
for(sim_ext in Simu_Ext){
  lapply(sim_ext, FUN=storeSim, method_=method)
}}

```


### Prior scaling 

Before we analyze any profile with any given segmentation method we scale it using
a differenced-based estimation of the variance (relying on the MAD).
The code is below.

```{r normalize_variance}
varDiff <- function(x_, method_='MAD'){
  n = length(x_)
  if(method_ == "MAD"){
	return(mad(diff(x_)/sqrt(2)))	
  }
  if(method_ == "HALL"){
    wei <- c(0.1942, 0.2809, 0.3832, -0.8582)
    mat <- wei %*% t(x_)
    mat[2, -n] = mat[2, -1]
    mat[3, -c(n-1, n)] = mat[3, -c(1, 2)]
    mat[4, -c(n-2, n-1, n)] = mat[4, -c(1, 2, 3)]   
    return(sqrt(sum(apply(mat[, -c(n-2, n-1, n)], 2, sum)^2) / (n-3)))
  }
}

```

## Segmentation Approaches

We provide some code for various segmentation approaches.

```{r}
Seg.method <- list()
i <- 0
```



### PELT

#### PELT default

```{r PELT_Default_MBIC}
# i <- i+1
# library(changepoint)
# Seg.method[[i]] <- list()
# Seg.method[[i]]$Name <- "Plt-Def"
# Seg.method[[i]]$FUN <- function(x){
#   w <- cpt.mean(x,method="PELT")
#   w.cpt <- c(cpts(w), length(x))
#      
#   return(list(cpt=w.cpt))
# }

```

#### PELT mBIC

```{r PELT_MBIC}
# i <- i+1
# library(changepoint)
# Seg.method[[i]] <- list()
# Seg.method[[i]]$Name <- "Plt-Mbic"
# Seg.method[[i]]$FUN <- function(x){
#     sumstat<-matrix(c(cumsum(c(0,x)),cumsum(c(0,x^2)),cumsum(c(0,(x-mean(x))^2))),ncol=3)
#     w<-PELT(sumstat,pen=2*log(length(x)),cost_func="mean.norm.mbic")
#     return(list(cpt=w$cpts))
# }
```

### FPOP

#### FPOP-2log(n)

```{r FPOP_default}
i <- i+1
library(fpop)
Seg.method[[i]] <- list()
Seg.method[[i]]$Name <- "Fpop-Def"
Seg.method[[i]]$FUN <- function(x){
  penalty <- 2*log(length(x))
  res <- Fpop(x, lambda=penalty)
  return( list(cpt=res$t.est) )
}

```

#### RFPOP-2log(n)

```{r RFPOP_default}
i <- i+1
library(robseg)
Seg.method[[i]] <- list()
Seg.method[[i]]$Name <- "RFpop-Def"
Seg.method[[i]]$FUN <- function(x){
  penalty <- 2*log(length(x))
  res <- Rob_seg.std(x, loss="Outlier", lambda=penalty, lthreshold = 3)
  return( list(cpt=res$t.est) )
}

```


#### FPOP-CROPS-Cap

```{r FPOP_CROPS_Cap}
i <- i+1
library(capushe)
source("CROPs.R")
Seg.method[[i]] <- list()
Seg.method[[i]]$Name <- "Fpop-CrCp"
Seg.method[[i]]$FUN <- function(x){
  n <- length(x)
  res <- CROPS.FPOP(x, min= 1/20*log(length(x)), max=100*log(length(x)))
  ## Capushe needs at least 10 models...
  dataCapushe <- data.frame(name=res[[1]][3,],
  pen.shape=lchoose(n-1,res[[1]][3,]),
  complexity=res[[1]][3,], contrast=res[[1]][4,])

  dataCapushe <- dataCapushe[order(dataCapushe$name), ]
  dataCapushe <- dataCapushe[dataCapushe$name <= (n/2 -2), ]
  ## no big cap in terms of K...
  ie <- which(dataCapushe$name <= 500)
  dataCapushe <- dataCapushe[ie , ]


  KC <- try(DDSE(dataCapushe, pct=0.2))
  KJ <- Djump(dataCapushe)
  if(class(KC) == "try-error"){ K <- as.integer(KJ@model) } else { K <- as.integer(KC@model) }
  index=(1:dim(res[[1]])[2])[res[[1]][3,]==K]

  return( list(cpt=res[[2]][[index]]) )
}
```

#### FPOP-CROPS-mBIC

```{r FPOP_CROPS_mBIC}
i <- i+1
source("CROPs.R")
Seg.method[[i]] <- list()
Seg.method[[i]]$Name <- "Fpop-CrmBIC"
Seg.method[[i]]$FUN <- function(x){
  n <- length(x)
  res <- CROPS.FPOP(x, min= 1/20*log(length(x)), max=100*log(length(x)))
  ## Capushe needs at least 10 models...
  Ks        <- res[[1]][3,]
  fit_value <- -0.5*res[[1]][4,]
  pen_value1 <- sapply(Ks, FUN=function(K){
     index <- (1:dim(res[[1]])[2])[res[[1]][3,]==K]
     cpt   <- res[[2]][[index]]
     lg <- diff(c(0, cpt))
     return(-0.5* sum(log(lg[!is.na(lg)])))
  })
  pen_value2 <- 0.5-(Ks)*log(n)     
 
  K <- Ks[which.max(fit_value+pen_value1+pen_value2)]
  index=(1:dim(res[[1]])[2])[res[[1]][3,]==K]

  return( list(cpt=res[[2]][[index]]) )
}
```

#### RFPOP-CROPS-Cap

```{r RFPOP_CROPS_Cap}
# i <- i+1
# library(capushe)
# source("CROPs.R")
# Seg.method[[i]] <- list()
# Seg.method[[i]]$Name <- "RFpop-CrCp"
# Seg.method[[i]]$FUN <- function(x){
#   n <- length(x)
#   res <- CROPS.RFPOP(x, min= 1/10*log(length(x)), max=100*log(length(x)))
#   ## Capushe needs at least 10 models...
#   dataCapushe <- data.frame(name=res[[1]][3,],
#   pen.shape=lchoose(n-1,res[[1]][3,]),
#   complexity=res[[1]][3,], contrast=res[[1]][4,])
# 
#   dataCapushe <- dataCapushe[order(dataCapushe$name), ]
#   dataCapushe <- dataCapushe[dataCapushe$name <= (n/2 -2), ]
#   ## no big cap in terms of K...
#   ie <- which(diff(dataCapushe$name) >= n/20)
#   if(length(ie) >= 1){
#     dataCapushe <- dataCapushe[1:min(ie), ]
#   }
# 
#   KC <- try(DDSE(dataCapushe))
#   KJ <- Djump(dataCapushe)
#   if(class(KC) == "try-error"){ K <- as.integer(KJ@model) } else { K <- as.integer(KC@model) }
#   index=(1:dim(res[[1]])[2])[res[[1]][3,]==K]
# 
#   return( list(cpt=res[[2]][[index]]) )
# }
```

### FDRseg


#### Smuce default

```{r SmuceR_Default}

i <- i+1
library(FDRSeg)
Seg.method[[i]] <- list()
Seg.method[[i]]$Name <- "FDRS-Smu"
Seg.method[[i]]$FUN <- function(x){
   if(length(x) <= 8000){
  res <- smuce(x)
  cpt <- c(res$left[-1]-1, length(x))
  return( list(cpt=cpt) ) } else {
  return( list(cpt=length(x)) )  }
}

```

#### FDRSeg Default

Not that for now we run FDRseg up $n=4000$.
For longer profiles is to slow for the debugging phase.
For longer profiles we simply output a 1 segment segmentation.

```{r FDRSeg_Default}
# as fdrseg is sometimes a bit long
# for the draft version only small profiles (less than 2000 datapoints)
# are analysed
i <- i+1
Seg.method[[i]] <- list()
Seg.method[[i]]$Name <- "FDRS-Def"
Seg.method[[i]]$FUN <- function(x){
 if(length(x) <= 4000){
 res <- fdrseg(x)
 cpt <- c(res$left[-1]-1, length(x))
 return( list(cpt=cpt) )
 } else {
 return( list(cpt=length(x)) )
 }
}

```


### pDPA/Fpsn + Capushe

#### Fpsn Kmax=100

```{r pDPA_Capushe_100}
i <- i+1
library(jointseg)
library(capushe)
Seg.method[[i]] <- list()
Seg.method[[i]]$Name <- "Fpsn-100"
Seg.method[[i]]$FUN <- function(x){
  Kmax <- min(trunc(length(x)/3), 100)
  res <- Fpsn(x, Kmax=Kmax)
  n <- length(x)
  dataCapushe <- data.frame(name=1:Kmax,
              pen.shape=lchoose(n-1,0:(Kmax-1)),
              complexity=1:Kmax, contrast=res$J.est)
  KC <- try(DDSE(dataCapushe, pct=0.2))
  KJ <- Djump(dataCapushe)
  if(class(KC) == "try-error"){ K <- as.integer(KJ@model) } else { K <- as.integer(KC@model) }

  return( list(cpt=res$t.est[K, 1:K]) )
}

```

#### Fpsn Kmax=400

```{r pDPA_Capushe_400}
i <- i+1
Seg.method[[i]] <- list()
Seg.method[[i]]$Name <- "Fpsn-400"
Seg.method[[i]]$FUN <- function(x){
  Kmax <- min(trunc(length(x)/3 ), 400)
  res <- Fpsn(x, Kmax=Kmax)
  n <- length(x)
  dataCapushe <- data.frame(name=1:Kmax,
            pen.shape=lchoose(n-1,0:(Kmax-1)),
            complexity=1:Kmax, contrast=res$J.est)

  KC <- try(DDSE(dataCapushe, pct=0.2))
  KJ <- Djump(dataCapushe)
  if(class(KC) == "try-error"){ K <- as.integer(KJ@model) } else { K <- as.integer(KC@model) }

  return( list(cpt=res$t.est[K, 1:K]) )
}

```

#### Fpsn mBIC Kmax=400

Here we use the mBIC after L2 optimisation.

```{r pDPA_mBIC_400}
i <- i+1
Seg.method[[i]] <- list()
Seg.method[[i]]$Name <- "Fpsn-mBIC_400"
Seg.method[[i]]$FUN <- function(x){
  Kmax <- min(trunc(length(x)/3 ), 400)
  res <- Fpsn(x, Kmax=Kmax)
  n <- length(x)
  ## assuming sigma=1
  ## mBIC not during optimisation but post
  fit_value <- -0.5*res$J.est 
  pen_value1 <- apply(res$t.est, 1, FUN=function(x) {
      lg <- diff(c(0, x))
     -0.5* sum(log(lg[!is.na(lg)]))
     })
  pen_value2 <- 0.5-(1:Kmax)*log(n)     
  
  K <- which.max(fit_value+pen_value1+pen_value2)
  return( list(cpt=res$t.est[K, 1:K]) )
}

```

#### Fpsn Kmax=5\sqrt(n)

```{r pDPA_Capushe_sqr}
#i <- i+1
#Seg.method[[i]] <- list()
#Seg.method[[i]]$Name <- "Fpsn-sqr"
#Seg.method[[i]]$FUN <- function(x){
#  Kmax <- min(trunc(length(x)/3 ), trunc(5*sqrt(length(x))) )
#  res <- Fpsn(x, Kmax=Kmax)
#  n <- length(x)
#  dataCapushe <- data.frame(name=1:Kmax,
#            pen.shape=lchoose(n-1,0:(Kmax-1)), 
#            complexity=1:Kmax, contrast=res$J.est)
  
#  KC <- try(DDSE(dataCapushe))
#  KJ <- Djump(dataCapushe)
#  if(class(KC) == "try-error"){ K <- as.integer(KJ@model) } else { K <- as.integer(KC@model) }
 
#  return( list(cpt=res$t.est[K, 1:K]) )
#}

```




### Segmentor 

#### Kmax=100 

```{r Seg_Default}

#i <- i+1
#library(Segmentor3IsBack)
#Seg.method[[i]] <- list()
#Seg.method[[i]]$Name <- "S3I-100"
#Seg.method[[i]]$FUN <- function(x){
#  res <- Segmentor(x, model=2, Kmax=100)
#  K <- SelectModel(res)
#  cpt <- res@breaks[K, 1:K]
#  return( list(cpt=cpt) )
#}

```

### Breakfast

#### TGUH

```{r Tguh_default}
i <- i+1
library(breakfast)
Seg.method[[i]] <- list()
Seg.method[[i]]$Name <- "Bft-Tgu"
Seg.method[[i]]$FUN <- function(x){
  w <- tguh.cpt(x)
  w.cpt <- c(sort(w$cpt), length(x))
  return(list(cpt=w.cpt))
}

```

#### Hybrid approach

```{r Hybrid_default}
i <- i+1
library(breakfast)
Seg.method[[i]] <- list()
Seg.method[[i]]$Name <- "Bft-Hyb"
Seg.method[[i]]$FUN <- function(x){
 w <- hybrid.cpt(x)
 w.cpt <- c(sort(w$cpt), length(x))
 return(list(cpt=w.cpt))
}

```

#### WBS 

```{r wbs_breakfast}
i <- i+1
library(breakfast)
Seg.method[[i]] <- list()
Seg.method[[i]]$Name <- "Bft-Wbs"
Seg.method[[i]]$FUN <- function(x){
 w <- wbs.cpt(x)
 w.cpt <- c(sort(w$cpt), length(x))
 return(list(cpt=w.cpt))
}

```

### WBS

#### WBS-sSIC

```{r wbs_ssic}
library(wbs)
i <- i+1
library(breakfast)
Seg.method[[i]] <- list()
Seg.method[[i]]$Name <- "Wbs-Ssic"
Seg.method[[i]]$FUN <- function(x){
     w <- wbs(x)
     w.cpt <- changepoints(w, penalty="ssic.penalty")
     w.cpt <- w.cpt$cpt.ic$ssic.penalty
     if(is.na(w.cpt[1])) w.cpt <- c() else { w.cpt <- sort(w.cpt)}

     return( list( cpt=c(w.cpt, length(x)) ) )
}

```


#### WBS-Threshold

```{r wbs_thres_1}
library(wbs)
i <- i+1
library(breakfast)
Seg.method[[i]] <- list()
Seg.method[[i]]$Name <- "Wbs-Th1"
Seg.method[[i]]$FUN <- function(x){
     w <- wbs(x)
     w.cpt <- changepoints(w)
     w.cpt <- w.cpt$cpt.th[[1]]
     if(is.na(w.cpt[1])) w.cpt <- c() else { w.cpt <- sort(w.cpt)}
   return( list( cpt=c(w.cpt, length(x)) ) )
}

```

#### SBS-Threshold
```{r sbs_thres_1}
library(wbs)
i <- i+1
library(breakfast)
Seg.method[[i]] <- list()
Seg.method[[i]]$Name <- "Sbs-Th1"
Seg.method[[i]]$FUN <- function(x){
     w <- sbs(x)
     w.cpt <- changepoints(w)
     w.cpt <- w.cpt$cpt.th[[1]]
     if(is.na(w.cpt[1])) w.cpt <- c() else { w.cpt <- sort(w.cpt)}
   return( list( cpt=c(w.cpt, length(x)) ) )
}

```

## Assesment measure

We first define a set of functions to recover:

- the true number of segments
- the true segment of each datapoints
- the true signal

```{r trueSmt}
############################
trueK <- function(sim){
  return(sim$Ktrue)
}
############################
trueSeg <- function(sim){
  rep(1:length(sim$mu), sim$Lg)
}
############################
trueSmt <- function(sim){
  rep(sim$mu, sim$Lg)
}

```

We then define a set of functions to recover:

- the estimated number of segments
- the estimated segment of each datapoints
- the estimated signal

```{r predictedSmt}
predictedK <- function(fit, x){
  length(fit$cpt)
}

predictedSeg <- function(fit, x){
  lgSeg <- diff(c(0, sort(fit$cpt)))
  segNb <- rep(1:length(lgSeg), lgSeg)
  return(segNb)
}

predictedSmt <- function(fit, x){
  lgSeg <- diff(c(0, sort(fit$cpt)))
  segNb <- rep(1:length(lgSeg), lgSeg)
  smt <- rep(by(x, segNb, FUN=mean)[1:length(lgSeg)], lgSeg)
  return(smt)
}
```

### Number of segments

We consider three simple statistics to assess whether
any given method is able to recover the true number of segments:

- selD: $\hat{K}-K^*$
- selE: $\mathbb{I}_{\hat{K}=K^*}$
- selMs: $\mathbb{I}_{\hat{K}<K^*}$
- selPs: $\mathbb{I}_{\hat{K}>K^*}$

```{r Khat}
#########################################################
selD <- function(sim, fit, x){
  sc <- predictedK(fit, x) - trueK(sim)
  names(sc) <- "selD"
  return(sc)
}
#########################################################
selE <- function(sim, fit, x){
  sc <- predictedK(fit, x) == trueK(sim)
  names(sc) <- "selE"
  return(sc)
}
#########################################################
selMs <- function(sim, fit, x){
  sc <- predictedK(fit, x) < trueK(sim)
  names(sc) <- "selMs"
  return(sc)
}
#########################################################
selPs <- function(sim, fit, x){
  sc <- predictedK(fit, x) > trueK(sim)
  names(sc) <- "selPs"
  return(sc)
}

```

### Mean-squared Error 

Here is a function to compute the MSE and the MSE divided by the MSE
obtained when considering only 1 segment (MSE_N).

```{r MSE}
#########################################################
MSE <- function(sim, fit, x){
  sc <- mean(  (trueSmt(sim) - predictedSmt(fit, x))^2  )
  names(sc) <- "Mse"
  return(sc)
}
#########################################################
MSE_N <- function(sim, fit, x){
  sc   <- mean(  (trueSmt(sim) - predictedSmt(fit, x))^2  )
  sc_n <- mean(  (      x     -      mean(x)         )^2  )
  sc <- sc/sc_n
  names(sc) <- "Mse_N"
  return(sc)
}
```

### Segmentation scoring

#### NID

Here is a function to compute the NID (Normalize Information distance) between the true and estimated segmentation. A score 
of 0 means a perfect match between the two.

```{r nid_score}
library(aricode)
nidScore <- function(sim, fit, x){
  if(trueK(sim) > 1){
    sc <- NID(trueSeg(sim), predictedSeg(fit, x))
  } else {
    sc <- NA
  }
  names(sc) <- "Nid"
  return(sc)
}

```

#### ARI
Here is a function to compute the ARI between the true and estimated segmentation. A score of 1 means a perfect match between the true
and estimated segmentation.

```{r ari_score}
ariScore <- function(sim, fit, x){
  if(trueK(sim) > 1){
    sc <- ARI(trueSeg(sim), predictedSeg(fit, x))
  } else {
    sc <- NA
  }
  names(sc) <- "Ari"
  return(sc)
}
```

#### Froebenius norm 
Here is a function to compute the Froebenius norm between the true and estimated segmentation. 

```{r Fb_score}
froebScore <- function(sim, fit, x){
   
 truesegment <- trueSeg(sim)
 predsegment     <- predictedSeg(fit, x)
 lseg        <- diff(c(0, fit$cpt))
 i_order <- order(truesegment, predsegment, method = "radix") - 1L
 #contingency <- table(truesegment, predsegment)
 contingency <- sortPairs(truesegment, predsegment, spMat=TRUE)$spMat 
 mattruesegment <- matrix(rep(sim$Lg, length(lseg)), ncol=length(lseg), byrow=FALSE)
 matsegment <- matrix(rep(lseg, length(sim$Lg)), nrow=length(sim$Lg), byrow=TRUE)

 
 part_Diff <- sum( contingency^2 * (1/mattruesegment - 1/matsegment)^2)
 part_Zero_truesegment <-  trueK(sim) - sum( contingency^2 * (1/mattruesegment)^2)
 part_Zero_segment <-predictedK(fit, x) - sum( contingency^2 * (1/matsegment)^2)

 sc <- sqrt(part_Diff+part_Zero_truesegment+part_Zero_segment)
  
 
 names(sc) <- "Fb"
 return(sc)
}
```



### List of all scoring functions

We make a list of all these scoring functions.

```{r all_scores}
Sco.method <- list()
i <- 1
Sco.method[[i]] <- MSE            ; i <- i+1
Sco.method[[i]] <- MSE_N          ; i <- i+1
Sco.method[[i]] <- selE           ; i <- i+1
Sco.method[[i]] <- froebScore     ; i <- i+1

Sco.method[[i]] <- selD           ; i <- i+1
Sco.method[[i]] <- selMs          ; i <- i+1
Sco.method[[i]] <- selPs          ; i <- i+1
Sco.method[[i]] <- nidScore       ; i <- i+1
Sco.method[[i]] <- ariScore       ; i <- i+1

```

## Segmentation and scoring


### Segmentation
We run all approches and assess their performances.


```{r load_simu_and_seg}
## Basics all ~ 4h30 with mc.cores=4
library(parallel)
one_simu_seg <- function(sim_, seg.method_, sim.method_){
  file_sim <- paste0("Simu_Profile/Error=", sim.method_$Name, "_", gsub(",[ ]", "_", sim_$Name), ".RData")
  file_seg <- paste0("Seg_Approach/Error=", sim.method_$Name, "_", gsub(",[ ]", "_", sim_$Name),  "_", seg.method_$Name, ".RData")

  ## run the approach only if not run already
  if(file.exists(file_sim)){
  if(!file.exists(file_seg)){
    load(file=file_sim)
    output.met <- mclapply(1:ncol(data_sim), FUN=function(i){
      x_ <- data_sim[, i]/varDiff(data_sim[, i])
      runtime <- system.time(res <- seg.method_$FUN(x_))[3]
      return(list(res=res, runtime=runtime))
      }, mc.cores=mc.cores)

    save(output.met, file=file_seg)

  }} else {cat("Generate simulation dataset first")}
}

###
for(sim.met in Sim.method){
  for(sim_ext in Simu_Ext){
    for(seg.met in Seg.method){
      lapply(sim_ext, FUN=one_simu_seg, seg.method_=seg.met,  sim.method_=sim.met)
}}}

```


### Scoring
```{r load_seg_and_score}
# sim_=Simu_Ext[[1]][[1]]; seg.method_=Seg.method[[1]]; sim.method_=Sim.method[[1]]; sco.methods_=Sco.method
# one_simu_score(sim_=Simu_Ext[[1]][[1]], seg.method_=Seg.method[[1]], sim.method_=Sim.method[[1]], sco.methods_=Sco.method)
one_simu_score <- function(sim_, seg.method_, sim.method_, sco.methods_){
  file_sim <- paste0("Simu_Profile/Error=", sim.method_$Name, "_", gsub(",[ ]", "_", sim_$Name), ".RData")
  file_seg <- paste0("Seg_Approach/Error=", sim.method_$Name, "_", gsub(",[ ]", "_", sim_$Name),  "_", seg.method_$Name, ".RData")
  file_sco <- paste0("Sco_Approach/Error=", sim.method_$Name, "_", gsub(",[ ]", "_", sim_$Name),  "_", seg.method_$Name, ".RData")
  ## run the approach only if not run already
  if(file.exists(file_seg)){
  if(!file.exists(file_sco)){
    load(file=file_seg)
    load(file=file_sim)

    output.sco <- do.call(rbind,
        mclapply(1:length(output.met), FUN=function(i){
          x <- c(i, output.met[[i]]$runtime, sapply(sco.methods_, FUN=function(fun)
                fun(sim=sim_, fit=output.met[[i]]$res, x=data_sim[, i]))
                )

          names(x)[1:2] <- c("sim_num", "t")
          return(x)
        }, mc.cores=mc.cores)
    )

    save(output.sco, file=file_sco)

  }} else {cat("Run segmentation first")}
}


##


##
## sim.met = Sim.method[[1]]; sim_ext = Simu_Ext[[1]]; seg.met = Seg.method[[1]]
for(sim.met in Sim.method){
  for(sim_ext in Simu_Ext){
    for(seg.met in Seg.method){
      lapply(sim_ext, FUN=one_simu_score, seg.method_=seg.met, sim.method_=sim.met, sco.methods_=Sco.method)
    }}}

```

### Summarized results

We first define some functions to summarize the results.

```{r summarization_function}
library(knitr)
library(kableExtra)

one_load_aggregate <- function(sim_, seg.method_, sim.method_){
  file_sco <- paste0("Sco_Approach/Error=", sim.method_$Name, "_", gsub(",[ ]", "_", sim_$Name),  "_", seg.method_$Name, ".RData")
  load(file_sco)
  data.frame(met=seg.method_$Name, t(signif(colMeans(output.sco[, c(2, 3, 5, 11)]),2)))
}
#one_load_aggregate(Simu_Ext[[1]][[1]], Seg.method[[4]], Sim.method[[1]])
all_load_aggregate <- function(sim_, Seg.methods_, sim.method_){
  do.call(rbind, lapply(Seg.methods_, FUN=function(met) one_load_aggregate(sim_=sim_, seg.method_=met, sim.method_=sim.method_)))
}
#all_load_aggregate(Simu_Ext[[1]][[1]], Seg.method, Sim.method[[1]])

```

#### Gaussian errors

Here are the results for a Gaussian error.

```{r table_summarization_gauss, results='asis'}
for(sim_ext in Simu_Ext){
    nb_lg <- length(sim_ext)
    data_to_show <- lapply(sim_ext, FUN=function(sim) all_load_aggregate(sim, Seg.methods_=Seg.method, sim.method_ = Sim.method[[1]]))
    nb_so <- ncol(data_to_show[[1]])
    icol <- rep(2:nb_so, each=nb_lg) + ((1:nb_lg) -1)*nb_so
    subname <- sapply(sim_ext, FUN=function(sim) paste0("_(x", gsub(".*x|,.*", "", sim$Name), ")"))
    for(i in 1:length(sim_ext)){
      colnames(data_to_show[[i]]) <- paste0(colnames(data_to_show[[i]]), subname[i])
    }
    data_to_show <- do.call(cbind, data_to_show)[, c(1, icol)]
    colnames(data_to_show)[1] <- "Met"

     essai <- kable(data_to_show, row.names = FALSE,  align="c", format="html", 
                    caption=paste0(sim_ext[[1]]$Name, ", ", Sim.method[[1]]$Name) ) %>%
              kable_styling(bootstrap_options = c("hover", "striped")) %>%
              add_header_above(c(" " = 1, "Time" = nb_lg, "MSE" = nb_lg, "E(K=K*)" = nb_lg, "ARI" = nb_lg)) %>%
              column_spec((1:nb_so-1)*nb_lg+1, border_right = T) %>% 
              row_spec(c(1, 6, 10), bold = T, color = "white", background = "#D7261E")
     print(essai)
  }
    
```

#### Student error

Here are the results for Student error.

```{r table_summarization_stu-10, results='asis'}

for(sim_ext in Simu_Ext){
    nb_lg <- length(sim_ext)
    data_to_show <- lapply(sim_ext, FUN=function(sim) all_load_aggregate(sim, Seg.methods_=Seg.method, sim.method_ = Sim.method[[2]]))
    nb_so <- ncol(data_to_show[[1]])
    icol <- rep(2:nb_so, each=nb_lg) + ((1:nb_lg) -1)*nb_so
    subname <- sapply(sim_ext, FUN=function(sim) paste0("_(x", gsub(".*x|,.*", "", sim$Name), ")"))
    for(i in 1:length(sim_ext)){
      colnames(data_to_show[[i]]) <- paste0(colnames(data_to_show[[i]]), subname[i])
    }
    data_to_show <- do.call(cbind, data_to_show)[, c(1, icol)]
    colnames(data_to_show)[1] <- "Met"

     essai <- kable(data_to_show, row.names = FALSE,  align="c", format="html", 
                    caption=paste0(sim_ext[[1]]$Name, ", ", Sim.method[[2]]$Name) ) %>%
              kable_styling(bootstrap_options = c("hover", "striped")) %>%
              add_header_above(c(" " = 1, "Time" = nb_lg, "MSE" = nb_lg, "E(K=K*)" = nb_lg, "ARI" = nb_lg)) %>%
              column_spec((1:nb_so-1)*nb_lg+1, border_right = T) %>% 
              row_spec(c(1, 6, 10), bold = T, color = "white", background = "#D7261E")
     print(essai)
  }
    
```
    
### ARMA error

Here are the results for an ARMA error.

```{r table_summarization_ARMA, results='asis'}

for(sim_ext in Simu_Ext){
    nb_lg <- length(sim_ext)
    data_to_show <- lapply(sim_ext, FUN=function(sim) all_load_aggregate(sim, Seg.methods_=Seg.method, sim.method_ = Sim.method[[3]]))
    nb_so <- ncol(data_to_show[[1]])
    icol <- rep(2:nb_so, each=nb_lg) + ((1:nb_lg) -1)*nb_so
    subname <- sapply(sim_ext, FUN=function(sim) paste0("_(x", gsub(".*x|,.*", "", sim$Name), ")"))
    for(i in 1:length(sim_ext)){
      colnames(data_to_show[[i]]) <- paste0(colnames(data_to_show[[i]]), subname[i])
    }
    data_to_show <- do.call(cbind, data_to_show)[, c(1, icol)]
    colnames(data_to_show)[1] <- "Met"

     essai <- kable(data_to_show, row.names = FALSE,  align="c", format="html", 
                    caption=paste0(sim_ext[[1]]$Name, ", ", Sim.method[[3]]$Name) ) %>%
              kable_styling(bootstrap_options = c("hover", "striped")) %>%
              add_header_above(c(" " = 1, "Time" = nb_lg, "MSE" = nb_lg, "E(K=K*)" = nb_lg, "ARI" = nb_lg)) %>%
              column_spec((1:nb_so-1)*nb_lg+1, border_right = T)  %>% 
              row_spec(c(1, 6, 10), bold = T, color = "white", background = "#D7261E")
     print(essai)
  }
```
